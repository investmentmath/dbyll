I"¸0<p><em>This post introduces Gaussian processes, i.e.Â processes with Gaussian finite dimensional multivariate distributions. Amongst Gaussian processes, the Ornstein Uhlenbeck process is the only Markovian covariance stationary example. It plays a key role in applications thanks to its tractability.</em></p>
<hr />
<h1 id="gaussian-processes">Gaussian processes</h1>
<p>Gaussian processes provide a very handy toolbox for modeling. A Gaussian process is a process <span class="math inline">\((X_{t})_{t \in \mathbb{T}}\)</span> such that for any finite set of indices <span class="math inline">\((t_{1},t_{2},\ldots,t_{n})\)</span>, <span class="math inline">\((X_{t_{1}},X_{t_{2}}\ldots,X_{t_{n}})\)</span> follows a multivariate Gaussian distribution. It should be reminded that the distribution of a random process, i.e.Â the distribution of the function <span class="math inline">\(\omega \mapsto (X_{t}(\omega))_{t \in \mathbb{T}}\)</span> , is determined by its finite dimensional distributions., i.e.Â the set of distributions corresponding to the set of functions <span class="math inline">\(\omega \mapsto (X_{t_{1}}(\omega),X_{t_{2}}(\omega)\ldots,X_{t_{n}}(\omega))\)</span> indexed by <span class="math inline">\(n-\)</span>uples <span class="math inline">\((t_{1},t_{2},\ldots,t_{n})\)</span>.</p>
<p>A multivariate Gaussian distribution is characterized by its mean <span class="math inline">\({\boldsymbol \mu}\)</span> (a vector) and its covariance matrix <span class="math inline">\({\boldsymbol \Sigma}\)</span>. All its other moments are a function of the first two moments.<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a></p>
<p>One can prove furthermore that the set of finite dimensional Gaussian distributions of a Gaussian process is characterized by the mean function <span class="math inline">\(t \mapsto E(X_{t})\)</span> and the covariance function <span class="math inline">\((u,v) \mapsto Cov(X_{u},X_{v})\)</span>. Indeed, one can recover the finite dimensional means and covariances from these two functions.</p>
<p>Brownian stochastic integrals using deterministic integrands deliver continuous time Gaussian processes. Assume the function <span class="math inline">\(h(\cdot)\)</span> on <span class="math inline">\([0,\infty[\)</span> is square integrable when restricted to <span class="math inline">\([0,t]\)</span> (<span class="math inline">\(\int_{0}^{t}|h(u)|^{2}du&lt;+\infty\)</span>). We can then define a continuous Gaussian process through varying the end point <span class="math inline">\(t\)</span> of the stochastic integral:<span class="math display">\[\int_{0}^{t}h(u)dW_{u}.\]</span> This process is obviously centered (<span class="math inline">\(E[\int_{0}^{t}h(u)dW_{u}]=0\)</span>). It is also Gaussian. When <span class="math inline">\(h\)</span> is a step function, it is a simple weighted sum (with deterministic weights) of non overlapping (and thus independent) Brownian increments. A square integrable function can be approximated in the <span class="math inline">\(L^{2}\)</span> sense by step functions <span class="math inline">\(h_{n}\)</span> and it can be shown that the limiting operation preserves the Gaussian nature of the process<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a>.</p>
<p>One can similarly define <span class="math inline">\(\int_{-\infty}^{t}h(u)dW_{u}\)</span> provided <span class="math inline">\(\int_{-\infty}^{t}|h(u)|^{2}du &lt; +\infty\)</span>.</p>
<h1 id="ornstein-uhlenbeck">Ornstein-Uhlenbeck</h1>
<p>A process is stationary if the distribution of <span class="math inline">\((X_{t+t_{1}},X_{t+t_{2}}\ldots,X_{t+t_{n}})\)</span> is independent of <span class="math inline">\(t\)</span> for all choices of <span class="math inline">\(n\)</span>, <span class="math inline">\(t\)</span> and <span class="math inline">\((t_{1},t_{2},\ldots,t_{n})\)</span>. Stationarity is invariance with respect to time translations. The covariance function <span class="math inline">\((u,v) \mapsto Cov(X_{u},X_{v})\)</span> then only depends on <span class="math inline">\(v-u\)</span>. A process is Markovian if as of time <span class="math inline">\(s\)</span>, all information regarding the future trajectory of <span class="math inline">\(X\)</span> is contained in the value <span class="math inline">\(X_{s}\)</span>. One does not need to know more than the current value of <span class="math inline">\(X\)</span> to predict its future.</p>
<p>Markovianity and stationarity imply<a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a> that <span class="math inline">\(cov(X_{s},X_{t})=a\exp(-b|t-s|)\)</span> for positive constants <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span>. This is the covariance function of the Ornstein Uhlenbeck which can thus be described as the only stationary markovian Gaussian process out there.</p>
<p>We can express the Ornstein Uhlenbeck process as Brownian integral. To do so, we need recover this covariance function through a suitable choice of <span class="math inline">\(h\)</span> in <span class="math inline">\(\int_{-\infty}^{t}h(t,u)dW_{u}\)</span> using: <span class="math display">\[cov(\int_{-\infty}^{s}h(s,u)dW_{u},\int_{-\infty}^{t}h(t,u)dW_{u})=\int_{-\infty}^{\min(s,t)}h(s,u)h(t,u)du.\]</span> Choosing <span class="math inline">\(h(t,u)=\beta^{\frac{1}{2}}\exp(-\alpha (t-u))\)</span> with both <span class="math inline">\(\beta\)</span> and <span class="math inline">\(\alpha\)</span> strictly positive, we get: <span class="math display">\[\int_{-\infty}^{\min(s,t)}h(s,u)h(t,u)du=\frac{\beta}{2\alpha}\exp(-\alpha|t-s|).\]</span></p>
<p>We have thus defined a Gaussian stationary and markovian process through:<span class="math display">\[X_{t}=\int_{-\infty}^{t}\beta^{\frac{1}{2}}\exp(-\alpha
(t-u))dW_{u}.\]</span> This is a moving average representation of the random variable <span class="math inline">\(X_{t}\)</span>, which has mean <span class="math inline">\(0\)</span> and variance <span class="math inline">\(\beta/(2\alpha)\)</span>. From the moving average representation, it is easy to establish that for <span class="math inline">\(t \geq s\)</span>: <span class="math display">\[X_{t}=\exp(-\alpha(t-s))X_{s}+\int_{s}^{t}\beta^{\frac{1}{2}}\exp(-\alpha
(t-u))dW_{u}.\]</span> We can condition the process on <span class="math inline">\(X_{0}=x\)</span> in which case we have: <span class="math display">\[X_{t}=\exp(-\alpha t)x+\int_{0}^{t}
\beta^{\frac{1}{2}}\exp(-\alpha (t-u))dW_{u}.\]</span> We can thus build a (non stationary) Ornstein Uhlenbeck process on <span class="math inline">\(\mathbb{R}_{+}\)</span>.</p>
<p>A useful martingale derived from <span class="math inline">\((X_{t})_{t \in \mathbb{r}_{+}}\)</span> is <span class="math inline">\((\exp(\alpha t)X_{t})_{t \in \mathbb{r}_{+}}\)</span>. Indeed: <span class="math display">\[\exp(\alpha
t)X_{t}=X_{0}+\int_{0}^{t}\beta^{\frac{1}{2}}\exp(\alpha
u)dW_{u}.\]</span> In differential form, this gives: <span class="math display">\[d(\exp(\alpha
t)X_{t})=\beta^{\frac{1}{2}}\exp(\alpha t)dW_{t},\]</span> or (ito): <span class="math display">\[\exp(\alpha t)dX_{t}+\alpha \exp(\alpha
t)X_{t}dt=\beta^{\frac{1}{2}}\exp(\alpha t)dW_{t},\]</span> i.e.: <span class="math display">\[dX_{t}=-\alpha X_{t}dt+\beta^{\frac{1}{2}}dW_{t},\]</span> which is known as Langevinâ€™s equation.</p>
<p>One can change the mean of the Ornstein Uhlenbeck process. An Ornstein Uhlenbeck process with mean <span class="math inline">\(\mu\)</span> is obtained from <span class="math inline">\(X\)</span> by setting <span class="math inline">\(Y_{t}=X_{t}+\mu\)</span>.</p>
<p>An Ornstein Uhlenbeck in one dimension is thus determined through three parameters with the following interpretations:</p>
<ul>
<li><span class="math inline">\(\sigma=\beta^{\frac{1}{2}}\)</span> is the volatility of the process; it describes the elasticity of <span class="math inline">\(X_{t}\)</span> to the contemporaneous shock of the Brownian motion,</li>
<li><span class="math inline">\(\alpha\)</span> is the speed of mean reversion of <span class="math inline">\((X_{t})_{t \in \mathbb{t}}\)</span>; <span class="math inline">\(\alpha\)</span> determines how persistent an impact a given shock <span class="math inline">\(\beta^{\frac{1}{2}}dW_{u}\)</span> has on the future trajectory of <span class="math inline">\((X_{t})_{t \in \mathbb{t}}\)</span>,</li>
<li><span class="math inline">\(\mu\)</span> is the mean of <span class="math inline">\((X_{t})_{t \in \mathbb{t}}\)</span>.</li>
</ul>
<p>Finally, the stationary distribution of an Ornstein Uhlenbeck process is <span class="math inline">\(N(\mu,(\beta/2\alpha )^{\frac{1}{2}})\)</span></p>
<p>To complete this introduction, letâ€™s quote a relationship between the Ornstein Uhlenbeck process and time changed Brownian processes (see this <a href="/math/2013/05/29/quadratic-variation-and-stochastic-integration.html" title="quadratic variation and stochastic integration">post</a>). To simplify the formulas, letâ€™s assume <span class="math inline">\(\mu=0\)</span>. The case where <span class="math inline">\(\mu\)</span> is different from <span class="math inline">\(0\)</span> is then immediate. We saw that <span class="math inline">\((\exp(\alpha t)X_{t})_{t \in \mathbb{r}_{+}}\)</span> is a continuous martingale that can thus be expressed as a time changed Brownian motion. To identify this time change, one starts from the idea that the time changed Brownian motion should have the same quadratic variation as the continuous martingale. We thus start by computing the latter: <span class="math display">\[[\beta^{\frac{1}{2}}\int_{0}^{\cdot}\exp(\alpha
u)dW_{u}]_{t}=\int_{0}^{t}\beta \exp(2\alpha
u)du=\frac{\beta}{2\alpha} \int_{0}^{t}d(\exp(2\alpha u))=\]</span> <span class="math display">\[\frac{\beta}{2\alpha} \int_{0}^{\exp(2 \alpha t)}dv.\]</span> We can thus find (see Kallenberg[2002], theorem 18.4) a new Brownian motion <span class="math inline">\((B_{t})_{t \in \mathbb{r}_{+}}\)</span> initialized at <span class="math inline">\(0\)</span> such that: <span class="math display">\[\beta^{\frac{1}{2}}\int_{0}^{t}\exp(\alpha
u)dW_{u}=(\frac{\beta}{2\alpha})^{1/2}\int_{0}^{\exp(2\alpha
t)}dB_{v}=(\frac{\beta}{2\alpha})^{1/2}B(\exp(2\alpha t)).\]</span> It thus turns out that: <span class="math display">\[\exp(\alpha t)X_{t}=(\beta/2\alpha
)^{\frac{1}{2}}B(\exp(2\alpha t)),\]</span> or: <span class="math display">\[X_{t}=(\beta/2\alpha )^{\frac{1}{2}}\exp(-\alpha
t)B(\exp(2\alpha t)).\]</span></p>
<p>Note: This note owes a lot to Kallenberg[2002], chapter 13.</p>
<p>References: Kallenberg, O., 2002, <em>Foundations of Modern Probability</em>, Springer.</p>
<h2 id="links">Links</h2>
<ul>
<li><a href="/assets/pdfs/2013-12-01-from-gaussian-to-ornstein-uhlenbeck-processes.pdf">Link to pdf</a></li>
</ul>
<section class="footnotes" role="doc-endnotes">
<hr />
<ol>
<li id="fn1" role="doc-endnote"><p>As a reminder, its density function is: <span class="math display">\[f({\boldsymbol
x})=(2\pi)^{-n/2}\det(\boldsymbol{\Sigma})^{-1/2}\exp(-\frac{1}{2}(\boldsymbol{x-\mu})^{t}\boldsymbol{\Sigma}^{-1}(\boldsymbol{(x-\mu})),\]</span> and its characteristic function is: <span class="math display">\[E(\exp(i\boldsymbol{u^{t}X}))=\exp(i\boldsymbol{u^{t}\mu}-\frac{1}{2}\boldsymbol{u^{t}\Sigma
u}).\]</span><a href="#fnref1" class="footnote-back" role="doc-backlink">â†©ï¸Ž</a></p></li>
<li id="fn2" role="doc-endnote"><p>This proof requires a few steps. It hinges crucially on the fact that for fixed <span class="math inline">\(t\)</span>, <span class="math inline">\(\int_{0}^{t}h_{n}(u)dW_{u}\)</span> converges to a Gaussian variable as <span class="math inline">\(n\)</span> tends to infinity. It also uses the fact that the process <span class="math inline">\((\int_{0}^{t}h(u)dW_{u})_{t \in \mathbb{T}}\)</span> has independent increments - remember that for two non overlapping intervals <span class="math inline">\(Cov(\int_{I}h(u)dW_{u},\int_{J}h(u)dW_{u})=0\)</span> which implies that both integrals are independent since they have Gaussian distributions.<a href="#fnref2" class="footnote-back" role="doc-backlink">â†©ï¸Ž</a></p></li>
<li id="fn3" role="doc-endnote"><p>See Kallenberg[2002], proposition <span class="math inline">\(13.7\)</span> p.Â 254.<a href="#fnref3" class="footnote-back" role="doc-backlink">â†©ï¸Ž</a></p></li>
</ol>
</section>
:ET