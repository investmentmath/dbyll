I"ÿ3<p><em>This post introduces the Ito rule of stochastic calculus. This rule provides the semimartingale decomposition of a nonlinear (this being the non trivial case) function of given semimartingales. Its simplest incarnation is the product rule of stochastic calculus which gives the semimartingale decomposition of the product of two given semimartingales.</em></p>
<hr />
<h1 id="ito-calculus-for-simple-polynomial-expressions">Ito calculus for simple polynomial expressions</h1>
<p>All semimartingales considered below are assumed continuous.</p>
<p>In <a href="/math/2013/05/29/quadratic-variation-and-stochastic-integration.html" title="QUADRATIC VARIATION AND STOCHASTIC INTEGRATION">this post</a> (I will use the same notation), we established the following algebraic identity : <span class="math display">\[X_{t}^{2}-X_{0}^{2}=\sum_{i=1}^{k}2X_{t_{i-1}}(X_{t_{i}}-X_{t_{i-1}})+\sum_{i=1}^{k}(X_{t_{i}}-X_{t_{i-1}})^{2},\]</span> along a grid <span class="math inline">\(\pi_{[0,t]}\)</span> satisfying <span class="math inline">\(t_{0}=0 ,t_{k}=t\)</span>. Applying the polarization identity<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a> we can derive : <span class="math display">\[X_{t}Y_{t}-X_{0}Y_{0}=\sum_{i=1}^{k}Y_{t_{i-1}}(X_{t_{i}}-X_{t_{i-1}})+\sum_{i=1}^{k}X_{t_{i-1}}(Y_{t_{i}}-Y_{t_{i-1}})+\]</span> <span class="math display">\[\sum_{i=1}^{k}(X_{t_{i}}-X_{t_{i-1}})(Y_{t_{i}}-Y_{t_{i-1}}).\]</span> We will concentrate on this last relationship since it contains the other one as a special case.</p>
<p>The terms on the right hand side have the following behavior as a function of the nature of <span class="math inline">\((X_{t})_{t \in [0,T]}\)</span> when the grid gets refined:</p>
<ul>
<li>If <span class="math inline">\((X_{t})_{t \in [0,T]}\)</span> and <span class="math inline">\((Y_{t})_{t \in [0,T]}\)</span> are continuous local martingales, the last term converges to the cross variation <span class="math inline">\(([X,Y]_{t})_{t \in [0,T]}\)</span> and the other terms converge to stochastic integrals. We have : <span class="math display">\[X_{t}Y_{t}-X_{0}Y_{0}=\int_{0}^{t}X_{u}dY_{u}+\int_{0}^{t}Y_{u}dX_{u}+[X,Y]_{t}.
\tag{1}\]</span></li>
<li>If <span class="math inline">\((X_{t})_{t \in [0,T]}\)</span> and <span class="math inline">\((Y_{t})_{t \in [0,T]}\)</span> are continuous bounded variation processes, the last term converges to<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a> <span class="math inline">\(0\)</span> and the first two terms converge towards the obvious Lebesgue Stieltjes integrals. Thus : <span class="math display">\[X_{t}Y_{t}-X_{0}Y_{0}=\int_{0}^{t}X_{u}dY_{u}+\int_{0}^{t}Y_{u}dX_{u}.
\tag{2}\]</span></li>
</ul>
<p>We can also study what happens when one of the process is a local continuous martingale, say <span class="math inline">\((X_{t})_{t \in [0,T]}\)</span> while the other one, say <span class="math inline">\((Y_{t})_{t \in [0,T]}\)</span>, is continuous bounded variation process. In this case, on can make use the following identity : <span class="math display">\[X_{t}Y_{t}-X_{0}Y_{0}=\sum_{i=1}^{k}X_{t_{i}}(Y_{t_{i}}-Y_{t_{i-1}})+\sum_{i=1}^{k}Y_{t_{i-1}}(X_{t_{i}}-X_{t_{i-1}}).\]</span> The dominated convergence theorem of Lebesgue Stieltjes integration shows that the first term converges to the Lebesgue Stieltjes integral<a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a> <span class="math inline">\((\int_{0}^{t}X_{u}dY_{u})_{t \in [0,T]}\)</span> while the second term, as usual, converges to the stochastic integral <span class="math inline">\((\int_{0}^{t}Y_{u}dX_{u})_{t \in [0,T]}\)</span>. We thus get : <span class="math display">\[X_{t}Y_{t}-X_{0}Y_{0}=\int_{0}^{t}X_{u}dY_{u}+\int_{0}^{t}Y_{u}dX_{u}.
\tag{3}\]</span></p>
<p>It is convenient to summarize these results in differential notation. The logic of the notation, illustrated on the product <span class="math inline">\(X_{t}Y_{t}\)</span> of two local martingales, is as follows. We write <span class="math inline">\(X_{t}Y_{t}-X_{0}Y_{0}=\int_{0}^{t}d (X_{u}Y_{u})\)</span>. Similarly, <span class="math inline">\([X,Y]_{t}=[X,Y]_{t}-[X,Y]_{0}=\int_{0}^{t}d[X,Y]_{u}\)</span>. We can then write equation <span class="math inline">\((1)\)</span> as : <span class="math display">\[d (X_{u}Y_{u})=X_{u}d
Y_{u}+Y_{u}d X_{u}+d[X,Y]_{u}.\]</span> We can wrap up the cases we saw as follows :</p>
<ul>
<li><span class="math inline">\((X_{t})_{t \in [0,T]}\)</span> and <span class="math inline">\((Y_{t})_{t \in [0,T]}\)</span> are local martingales : <span class="math display">\[d (X_{u}Y_{u})=X_{u}d Y_{u}+Y_{u}d
X_{u}+d[X,Y]_{u} \qquad (\text{equation}\,1).\]</span></li>
<li><span class="math inline">\((X_{t})_{t \in [0,T]}\)</span> and <span class="math inline">\((Y_{t})_{t \in [0,T]}\)</span> are of bounded variation : <span class="math display">\[d (X_{u}Y_{u})=X_{u}d Y_{u}+Y_{u}d
X_{u} \qquad (\text{equation}\,2).\]</span></li>
<li><span class="math inline">\((X_{t})_{t \in [0,T]}\)</span> is of bounded variation and <span class="math inline">\((Y_{t})_{t \in [0,T]}\)</span> is a local martingale : <span class="math display">\[d
(X_{u}Y_{u})=X_{u}d Y_{u}+Y_{u}d X_{u} \qquad
(\text{equation}\,3).\]</span>.</li>
</ul>
<p>In the special case where <span class="math inline">\(X\)</span> is equal to <span class="math inline">\(Y\)</span>, we have :</p>
<ul>
<li><span class="math inline">\((X_{t})_{t \in [0,T]}\)</span> is a local martingale : <span class="math inline">\(dX^{2}_{u}=2X_{u}dX_{u}+d[X]_{u}\)</span>.</li>
<li><span class="math inline">\((X_{t})_{t \in [0,T]}\)</span> is of bounded variation : <span class="math inline">\(dX^{2}_{u}=2X_{u}dX_{u}\)</span>.</li>
</ul>
<h1 id="from-product-rules-to-ito-calculus">From product rules to Ito calculus</h1>
<p>We now consider two continuous semimartingales <span class="math inline">\((X_{t})_{t \in [0,T]}\)</span> and <span class="math inline">\((Y_{t})_{t \in [0,T]}\)</span> with their decompositions : <span class="math display">\[X_{t}=M_{t}+A_{t},\]</span> <span class="math display">\[Y_{t}=N_{t}+B_{t},\]</span> into their local continuous martingale component (<span class="math inline">\((M_{t})_{t \in [0,T]}\)</span> and <span class="math inline">\((N_{t})_{t \in [0,T]}\)</span>) and their continuous bounded variation process (<span class="math inline">\((A_{t})_{t \in [0,T]}\)</span> and <span class="math inline">\((B_{t})_{t \in [0,T]}\)</span>). We can write : <span class="math display">\[d (X_{u}Y_{u})=X_{u}d Y_{u}+Y_{u}d
X_{u}+d[X,Y]_{u}\]</span> <span class="math display">\[=X_{u}d B_{u}+Y_{u}d A_{u}+X_{u}d
N_{u}+Y_{u}d M_{u}+d[X,Y]_{u},\]</span> with <span class="math inline">\([X,Y]_{u}=[M,N]_{u}\)</span>. We note that if <span class="math inline">\(X\)</span> or <span class="math inline">\(Y\)</span> is actually a bounded variation process, then <span class="math inline">\([X,Y]_{u}=0\)</span>.</p>
<p>This decomposition of <span class="math inline">\(d(X_{u}Y_{u})\)</span> is actually the semimartingale decomposition of <span class="math inline">\((X_{t}Y_{t})_{\in [0,T]}\)</span> into its local martingale component :<span class="math display">\[\int_{0}^{t}X_{u}d
N_{u}+\int_{0}^{t}Y_{u}d M_{u},\]</span> and its finite variation component : <span class="math display">\[\int_{0}^{t}X_{u}d B_{u}+\int_{0}^{t}Y_{u}d
A_{u}+[X,Y]_{t}.\]</span> We can thus compute <span class="math inline">\(d(X_{u}Y_{u}Z_{u})\)</span> for a third semimartingale <span class="math inline">\((Z_{t})_{t \in [0,T]}\)</span> by considering the product of the three semimartingale <span class="math inline">\(X_{u}Y_{u}Z_{u}\)</span> as a product of two semimartingales <span class="math inline">\((X_{u}Y_{u})Z_{u}\)</span>. This remark leads to consider the differential of polynomial functions of a set of underlying semimartingales.</p>
<p>One can show by recurrence along the above lines that for any polynomial expression <span class="math inline">\(F(X_{1,u},X_{2,u},\ldots,X_{n,u})\)</span> in <span class="math inline">\(n\)</span> semimartingales, we have : <span class="math display">\[dF(X_{1,u},\ldots,X_{n,u})=
\sum_{i}F^{\prime}_{i}(X_{1,u},\ldots,X_{n,u})dX_{i,u}+\]</span> <span class="math display">\[\frac{1}{2}\sum_{i,j}F^{\prime\prime}_{i,j}(X_{1,u},\ldots,X_{n,u})d[X_{i},X_{j}]_{u}.\]</span> The reader can check that this works for all simple polynomials considered in the first section, and that if it works for two polynomial expressions, it works for their product.</p>
<p>What then remains to be seen is whether this can be applied to general twice continuously differentiable functions. This is indeed the case. The key step in the proof of this result consists in restricting the semimartingales to compact subspaces through localization, and then approximate the function with polynomials. In a compact domain, a twice continuously differentiable function together with its first and second derivatives can be arbitrarily closely approximated (in the sense of uniform convergence) by a polynomial.</p>
<p>To conclude, letâ€™s state Itoâ€™s formula precisely :</p>
<p><strong>Theorem (Itoâ€™s Formula):</strong> <em>Consider <span class="math inline">\(n\)</span> continuous semimartingales <span class="math inline">\((X_{1,t},\ldots,X_{n,t})\)</span> with <span class="math inline">\(t\)</span> in <span class="math inline">\([0,T]\)</span> and a twice continuously differentiable function of <span class="math inline">\(n\)</span> variables <span class="math inline">\(F: \mathbb{R}^{n}\rightarrow \mathbb{R}\)</span>. Then we have with probability one for all <span class="math inline">\(u\)</span> in <span class="math inline">\([0,T]\)</span>:<span class="math display">\[dF(X_{1,u},\ldots,X_{n,u})=
\sum_{i}F&#39;_{i}(X_{1,u},\ldots,X_{n,u})dX_{i,u}+\]</span> <span class="math display">\[\frac{1}{2}\sum_{i,j}F^{\prime\prime}_{i,j}(X_{1,u},\ldots,X_{n,u})d[X_{i},X_{j}]_{u},\]</span> or in integral form, for all <span class="math inline">\(t\)</span> in <span class="math inline">\([0,T]\)</span>: <span class="math display">\[F(X_{1,t},\ldots,X_{n,t})-F(X_{1,0},\ldots,X_{n,0})=
\sum_{i}\int_{0}^{t}F&#39;_{i}(X_{1,u},\ldots,X_{n,u})dX_{i,u}+\]</span> <span class="math display">\[\frac{1}{2}\sum_{i,j}\int_{0}^{t}F^{\prime\prime}_{i,j}(X_{1,u},\ldots,X_{n,u})d[X_{i},X_{j}]_{u}.\]</span></em></p>
<p><span class="math inline">\({\scriptstyle \blacksquare}\)</span></p>
<p>Â </p>
<p>This theorem singles out the semimartingale decomposition of the process <span class="math inline">\((F(X_{1,t},\ldots,X_{n,t}))_{t \in [0,T]}\)</span>. If we decompose each semimartingale <span class="math inline">\((X_{i,t})_{t \in [0,T]}\)</span> into its bounded variation component <span class="math inline">\((A_{i,t})_{t \in [0,T]}\)</span> and its local martingale component <span class="math inline">\((M_{i,t})_{t \in [0,T]}\)</span>, then the bounded variation component of <span class="math inline">\((F(X_{1,t},\ldots,X_{n,t}))_{t \in [0,T]}\)</span> is : <span class="math display">\[
\sum_{i}\int_{0}^{t}F&#39;_{i}(X_{1,u},\ldots,X_{n,u})dA_{i,u}+\frac{1}{2}\sum_{i,j}\int_{0}^{t}F^{\prime\prime}_{i,j}(X_{1,u},\ldots,X_{n,u})d[X_{i},X_{j}]_{u},\]</span> while the local martingale component is: <span class="math display">\[
\sum_{i}\int_{0}^{t}F&#39;_{i}(X_{1,u},\ldots,X_{n,u})dM_{i,u}.\]</span></p>
<p>Note : This post takes its inspiration from section IV.32 of Rogers[2000].</p>
<p>Reference: Rogers L.C.G. and D. Williams[2000] : <em>Diffusions,Markov Processes and Martingales, Vol 2, Ito Calculus</em>, Cambridge University Press.</p>
<h2 id="links">Links</h2>
<ul>
<li><a href="/assets/pdfs/2013-10-07-ito-calculus.pdf">Link to pdf</a></li>
</ul>
<section class="footnotes" role="doc-endnotes">
<hr />
<ol>
<li id="fn1" role="doc-endnote"><p><span class="math inline">\(ab=\frac{1}{4}[(a+b)^{2}-(a-b)^{2}]\)</span>.<a href="#fnref1" class="footnote-back" role="doc-backlink">â†©ï¸Ž</a></p></li>
<li id="fn2" role="doc-endnote"><p>The cross variation of two bounded variation functions is zero. This can easily be deduced from the fact that the quadratic variation of each process is null and using the Cauchy Schwartz inequality.<a href="#fnref2" class="footnote-back" role="doc-backlink">â†©ï¸Ž</a></p></li>
<li id="fn3" role="doc-endnote"><p>In Lebesgue Stieltjes integration, the sums <span class="math inline">\(\sum_{i=1}^{k}X_{t_{i}}(Y_{t_{i}}-Y_{t_{i-1}})\)</span> and <span class="math inline">\(\sum_{i=1}^{k}X_{t_{i-1}}(Y_{t_{i}}-Y_{t_{i-1}})\)</span> converge to the same result, the integral <span class="math inline">\(\int_{0}^{t}X_{u}dY_{u}\)</span>. Within stochastic integration, i.e.Â when <span class="math inline">\((Y_{t})_{t \in [0,T]}\)</span> is a local martingale, the two sums differ. The second expression converges to the Ito integral <span class="math inline">\(\int_{0}^{t}X_{u}dY_{u}\)</span> while the first converges to <span class="math inline">\(\int_{0}^{t}X_{u}dY_{u}+[X,Y]_{t}\)</span>. The Stratonovitch integral is defined as the limit of the average of the two weighted sums, namely <span class="math inline">\(\int_{0}^{t}X_{u}dY_{u}+\frac{1}{2}[X,Y]_{t}\)</span>.<a href="#fnref3" class="footnote-back" role="doc-backlink">â†©ï¸Ž</a></p></li>
</ol>
</section>
:ET