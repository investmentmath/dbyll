I"ÀI<p><em>Stochastic integration was first developed in the context of the Brownian motion and the theory was then extended to martingales with continuous paths, which we will call continuous martingales in what follows. Continuous martingales have a lot of structure which eases the corresponding stochastic integration theory. The quadratic variation of a continuous martingale is the central concept in this theory. The purpose of this note is to provide an easy introduction to this subject before presenting Ito calculus in a later post.</em></p>
<hr />
<h1 id="the-quadratic-variation-process">The quadratic variation process</h1>
<p>The variation of order <span class="math inline">\(p\)</span> of a trajectory <span class="math inline">\((X_{t}(\omega))_{t \in [0,T]}\)</span> of the stochastic process <span class="math inline">\((X_{t})_{t \in [0,T]}\)</span> for a given partition <span class="math inline">\(\pi_{[0,T]}=(t_{0}=0,t_{1},\ldots,t_{n}=T)\)</span> of <span class="math inline">\([0,T]\)</span> is defined as: <span class="math display">\[V^{(p)}(\pi_{[0,T]})(\omega)=\sum_{k=1}^{n}|X_{t_{k}}(\omega)-X_{t_{k-1}}(\omega)|^{p}.\]</span> If <span class="math inline">\(V^{(p)}(\pi_{[0,T]})(\omega)\)</span> has a limit<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a> when the partitions get finer we can refer to it as <span class="math inline">\(V^{(p)}_{T}(\omega)\)</span>, the variation of order <span class="math inline">\(p\)</span> of trajectory <span class="math inline">\(\omega\)</span> over the time interval <span class="math inline">\([0,T]\)</span>.</p>
<p>For finite variation processes, we have <span class="math inline">\(V^{(1)}_{T}(\omega)&lt;\infty\)</span> and for <span class="math inline">\(p\&gt;1\)</span>, it turns out that<span class="math inline">\(V^{(p)}_{T}(\omega)_{T}=0\)</span>. This is the case for instance when <span class="math inline">\((X_{t}(\omega))_{t \in [0,T]}\)</span> is monotonic in time (increasing or decreasing) or when it can be written as the difference of two monotonic functions. Most usual functions are of this sort. In contrast, the continuous martingales have <span class="math inline">\(V^{(1)}_{T}(\omega)=+\infty\)</span>, <span class="math inline">\(V^{(2)}_{T}(\omega)&lt;\infty\)</span> and for <span class="math inline">\(p\&gt;2\)</span>, <span class="math inline">\(V^{(p)}(\omega)_{T}=0\)</span>. This is easily seen in the case of the Brownian motion, the prototypical continuous martingale, as we now show.</p>
<p>The Brownian motion has independent non overlapping increments <span class="math inline">\(B_{t}-B_{s} \, (t \geq s)\)</span>, with law <span class="math inline">\({\cal N}(0,t-s)\)</span>. In particular, we have:<span class="math display">\[E[(B_{t}-B_{s})^2]= t-s.\]</span> We can thus compute the expectation of <span class="math inline">\(V^{(2)}(\pi_{[0,T]})(\omega)\)</span>: <span class="math display">\[E[V^{(2)}(\pi_{[0,T]})(\omega)]=\sum_{k=1}^{n}|t_{k}-t_{k-1}|=T.\]</span> We can also compute the variance of <span class="math inline">\(V^{(2)}(\pi_{[0,T]})(\omega)\)</span>: <span class="math display">\[Var[V^{(2)}(\pi_{[0,T]})(\omega)]=E\left[\left(\sum_{k=1}^{n}|B_{t_{k}}(\omega)-B_{t_{k-1}}(\omega)|^{2}-(t_{k}-t_{k-1})\right)^{2}\right],\]</span> where the mean of the overall variation has been re-affected to the increments. Terms within the parenthesis are independent (the Brownian increments are independent) and centered. When developing the square, the cross products thus have expectation zero and we are left with :<span class="math display">\[Var[V^{(2)}(\pi_{[0,T]})(\omega)]=\sum_{k=1}^{n}E\left[|B_{t_{k}}(\omega)-B_{t_{k-1}}(\omega)|^{2}-(t_{k}-t_{k-1})\right]^{2}\]</span><span class="math display">\[=\sum_{k=1}^{n}(t_{k}-t_{k-1})^{2}E\left[(Z_{k}^{2}-1)^{2}\right],\]</span>where each <span class="math inline">\(Z_{k}\)</span> is has a standard normal distribution so that <span class="math inline">\(E\left[(Z_{k}^{2}-1)^{2}\right]=1\)</span> (cf.Â the properties of the chi-square distribution). We are thus left with:<span class="math display">\[Var[V^{(2)}(\pi_{[0,T]})(\omega)]=\sum_{k=1}^{n}(t_{k}-t_{k-1})^{2}\leq
T\max_{k}|t_{k}-t_{k-1}|.\]</span>The variance of the quadratic variation thus converges to zero as the grid gets finer. This means that the quadratic variation of the Brownian motion over <span class="math inline">\([0,T]\)</span> converges to <span class="math inline">\(T\)</span> in the <span class="math inline">\(L^{2}\)</span> sense as the grid gets finer and in that sense, we can say that the quadratic variation of the Brownian motion is not random:<span class="math display">\[\boxed{V_{T}^{(2)}(\omega)=T.}\]</span> This property almost characterizes the Brownian motion (cf.Â Levyâ€™s characterization theorem).This is at first sight a surprising result. It illustrates the fact that randomness can disappear through some sort of law of large numbers in the limit of continuous time. As a final observation, we note that for a given trajectory, the quadratic variation of the Brownian motion is a strictly increasing function of time.</p>
<p>We now wish to consider the more general case of continuous martingales and study their quadratic variation. Some elementary algebra reveals that: <span class="math display">\[X_{t_{i}}^{2}-X_{t_{i-1}}^{2}=2X_{t_{i-1}}(X_{t_{i}}-X_{t_{i-1}})+(X_{t_{i}}-X_{t_{i-1}})^{2}.\]</span> Cumulating this relationship along a grid <span class="math inline">\(\pi_{[0,T]}\)</span>, we get (<span class="math inline">\(k \le n\)</span>): <span class="math display">\[X_{t_{k}}^{2}-X_{0}^{2}=\sum_{i=1}^2X_{t_{i-1}}(X_{t_{i}}-X_{t_{i-1}})+\sum_{i=1}^{k}(X_{t_{i}}-X_{t_{i-1}})^{2}.\]</span> For a continuous martingale <span class="math inline">\((X_{t})_{t \in [0,T]}\)</span>, one can show that as the grid gets finer, the first term on the right hand side converges<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a> to a continuous martingale <span class="math inline">\((M_{t})_{t \in [0,T]}\)</span>, allowing to define a continuous process <span class="math inline">\((A_{t})_{t \in [0,T]}\)</span> through <span class="math inline">\(A_{t}=X_{t}^{2}-X_{0}^{2}-M_{t}\)</span> as the quadratic variation <span class="math inline">\(V^{(2)}([0,t])\)</span>. This process is increasing and as such, it is of bounded variation. It is usually noted <span class="math inline">\(([X]_{t})_{t \in [0,T]}\)</span>. Implicit in the notation <span class="math inline">\([X]_{t}\)</span> is the initialization at <span class="math inline">\(0\)</span>.</p>
<p>The decomposition: <span class="math display">\[\boxed{X_{t}^{2}-X_{0}^{2}=M_{t}+[X]_{t},}\]</span> is called the Doob-Meyer decomposition of the submartingale <span class="math inline">\(X_{t}^{2}-X_{0}^{2}\)</span> (see this <a href="/math/2013/05/25/introducing-diffusions.html" title="INTRODUCING DIFFUSIONS">other post</a>). We noted that <span class="math inline">\(([X]_{t})_{t \in [0,T]}\)</span> is increasing. One can in addition show that a continuous martingale having a constant quadratic variation process is constant. If we rule out processes that are constant on some interval of time, we therefore get continuous martingales with strictly increasing quadratic variation. The main difference with the Brownian motion is that in the latter case, quadratic variation is the deterministic function <span class="math inline">\(t\)</span>, whereas in the case of (â€˜nowhere constantâ€™) continuous martingales, quadratic variation is strictly increasing but potentially random. One can actually prove that such a continuous martingale is just a time-changed Brownian motion, where the time change works like a random clock which never goes backwards. Defining the time change through: <span class="math display">\[\tau(t)=\inf\{u:[X]_{u}\&gt;t\},\]</span> assuming this has a solution almost everywhere over an open interval <span class="math inline">\([0,h]\)</span>, i.e.Â almost everywhere <span class="math inline">\(h&lt;[X]_{T}\)</span>. The time changed process <span class="math inline">\((B_{t}=X_{\tau(t)})_{t \in [0,h]}\)</span> has quadratic variation <span class="math inline">\([B_{t}]=t\)</span> and one can show that it is a Brownian motion for the time changed filtration. In other words, continuous martingales are never far from the Brownian motion (see the end of the post for another way to recover a Brownian motion from a continuous local martingale).</p>
<h1 id="stochastic-integration-and-the-ito-isometry">Stochastic Integration and the Ito Isometry</h1>
<p>As explained in this <a href="/finance/2013/03/14/integrating-returns.html" title="INTEGRATING RETURNS">other post</a>, we could apply standard integration theory to make sense of expressions like <span class="math inline">\(\int_{0}^{T} H_{u}dX_{u}\)</span> provided the process <span class="math inline">\((X_{t})_{t \in [0,T]}\)</span> had trajectories of bounded first order variation. But this cannot be the case of a process of finite quadratic variation. Stochastic integration is therefore special. To make sense of an expression like: <span class="math display">\[\int_{0}^{T} H_{u}dX_{u},\]</span> the strategy consists in defining an approximation scheme whereby suitable processes <span class="math inline">\((H_{t})_{t \in [0,T]}\)</span> get approximated by simpler processes <span class="math inline">\((H^{n}_{t})_{t \in [0,T]}\)</span> for which <span class="math inline">\(\int_{0}^{T} H^{n}_{u}dX_{u}\)</span> is easily defined. The trick then is to ensure that <span class="math inline">\(\int_{0}^{T} H^{n}_{u}dX_{u}\)</span> converges in some well defined sense as <span class="math inline">\((H^{n}_{t})_{t \in [0,T]}\)</span> converges to <span class="math inline">\((H_{t})_{t \in [0,T]}\)</span>. The key to this approximating scheme is the existence of the quadratic variation of <span class="math inline">\((X_{t})_{t \in [0,T]}\)</span> (see the figure below).</p>
<p>To get a glimpse into this scheme, letâ€™s assume <span class="math inline">\(H\)</span> is a simple weighting scheme as defined in this <a href="/finance/2013/04/06/portfolio-returns.html" title="PORTFOLIO RETURNS">other post</a>. For such a weighting scheme, the stochastic integral is defined as: <span class="math display">\[\int_{0}^{t}H_{u}dX_{u}=\sum_{i=0}^{J(t)-1}H^{i}(X_{T_{i+1}}-X_{T_{i}})+H^{J(t)}(X_{t}-X_{T_{J(t)}}),\]</span> where <span class="math inline">\(J(t)(\omega)\)</span> is set such that <span class="math inline">\(T_{J(t)}(\omega) \leq t &lt; T_{J(t)+1}(\omega)\)</span>. We can now compute the square of the <span class="math inline">\(L^{2}\)</span> norm of this random variable: <span class="math display">\[E\left[\left(\int_{0}^{t}H_{u}dX_{u}\right)^{2}\right]=\]</span> <span class="math display">\[E\left[\left(\sum_{i=0}^{J(t)-1}H^{i}(X_{T_{i+1}}-X_{T_{i}})+H^{J(t)}(X_{t}-X_{T_{J(t)}})\right)^{2}\right].\]</span> The martingale property implies that in the development of the squared sum, all cross products have expectation <span class="math inline">\(0\)</span>. One then gets as a result the expression: <span class="math display">\[\sum_{i=0}^{J(t)-1}E\left[(H^{i}(X_{T_{i+1}}-X_{T_{i}}))^{2}\right]+E\left[(H^{J(t)}(X_{t}-X_{T_{J(t)}}))^{2}\right].\]</span> For one term we have: <span class="math display">\[E\left[(H^{i}(X_{T_{i+1}}-X_{T_{i}}))^{2}\right]=E\left[(H^{i})^{2}E_{T_{i}}\left[(X_{T_{i+1}}-X_{T_{i}})^{2}\right]\right],\]</span> but <span class="math display">\[E_{T_{i}}\left[(X_{T_{i+1}}-X_{T_{i}})^{2}\right]=E_{T_{i}}\left[X_{T_{i+1}}^{2}\right]-2E_{T_{i}}\left[X_{T_{i+1}}X_{T_{i}}\right]+E_{T_{i}}\left[X_{T_{i}}^{2}\right]\]</span> <span class="math display">\[=E_{T_{i}}\left[X_{T_{i+1}}^{2}-X_{T_{i}}^{2}\right]\]</span> <span class="math display">\[=E_{T_{i}}\left[[X]_{T_{i+1}}^{2}-[X]_{T_{i}}^{2}\right].\]</span> Using these building blocks, we get: <span class="math display">\[E\left[\left(\int_{0}^{t}H_{u}dX_{u}\right)^{2}\right]=\]</span> <span class="math display">\[E\left[\sum_{i=0}^{J(t)-1}(H^{i})^{2}([X]_{T_{i+1}}-[X]_{T_{i}})+(H^{J(t)})^{2}([X]_{t}-[X]_{T_{J(t)}})\right],\]</span> but this last term is the â€˜usualâ€™ integral of <span class="math inline">\(H\)</span> against the increasing process <span class="math inline">\([X]\)</span>. We can thus write:<span class="math display">\[E\left[\left(\int_{0}^{t}H_{u}dX_{u}\right)^{2}\right]=E\left[\int_{0}^{t}H_{u}^{2}d[X]_{u}\right].\]</span></p>
<p>This result, recast as: <span class="math display">\[\boxed{\left(E\left[\left(\int_{0}^{T}H_{u}dX_{u}\right)^{2}\right]\right)^{1/2}=\left(E\left[\int_{0}^{T}H_{u}^{2}d[X]_{u}\right]\right)^{1/2}.}\]</span> is called the Ito isometry property of stochastic integrals. It essentially shows that stochastic integration as a mapping from the simple weight function <span class="math inline">\((H_{t})_{t \in [0,T]}\)</span> to <span class="math inline">\(\int_{0}^{T}H_{u}dX_{u}\)</span> preserves a notion of norm, where the norm of <span class="math inline">\((H_{t})_{t \in [0,T]}\)</span> is defined as <span class="math display">\[\|H\|_{[X]}=\left(E\left[\int_{0}^{T}H_{u}^{2}d[X]_{u}\right]\right)^{1/2}\]</span> while the norm of the random variable <span class="math inline">\(\int_{0}^{T}H_{u}dX_{u}\)</span> is defined as <span class="math display">\[\|\int
HdX\|=\left(E\left[\left(\int_{0}^{T}H_{u}dX_{u}\right)^{2}\right]\right)^{1/2}.\]</span> Both spaces are so called <span class="math inline">\(L^{2}\)</span> spaces.</p>
<p>Using this isometry property, one can extend the stochastic integral beyond simple weights to â€˜progressively measurable<a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a> weightsâ€™ with finite norm. The nature of the norm on weight functions involves the quadratic variation function of the continuous martingale. It is thus not surprising to see that what counts in the most general formulation of stochastic integration versus continuous martingale is the finiteness for all <span class="math inline">\(t\)</span> and almost all events <span class="math inline">\(\omega\)</span> of: <span class="math display">\[\int_{0}^{t}H_{u}^{2}d[X]_{u}.\]</span> This is, together with progressive measurability, the key condition for stochastic integration to be meaningful.</p>
<p>The overall situation is summarized in the following figure: <a href="/assets/media/quadraticvariation.png"><img src="/assets/media/quadraticvariation.png" /></a></p>
<h1 id="the-quadratic-variation-of-a-stochastic-integral">The Quadratic Variation of a Stochastic Integral</h1>
<p>The reader who has made it so far will not be surprised by the following result: <span class="math display">\[[\int_{0}^{\cdot}H_{u}dX_{u}]_{t}=\int_{0}^{t}(H_{u})^{2}d[X]_{u},\]</span> which is easily guessed from the basic definition of the stochastic integral for simple weights. We can extend this result to covariation, which we now define.</p>
<p>We are given two continuous martingales <span class="math inline">\((X_{t})_{t \in [0,T]}\)</span> and <span class="math inline">\((Y_{t})_{t \in [0,T]}\)</span>. We can study their covariation <span class="math display">\[Cov(\pi_{[0,T]})(\omega)=\sum_{k=1}^{n}(X_{t_{k}}(\omega)-X_{t_{k-1}}(\omega))(Y_{t_{k}}(\omega)-Y_{t_{k-1}}(\omega)).\]</span> Fortunately, we do not need to work too hard now. Indeed, we can simply use the so-called polarization identity: <span class="math display">\[ab=\frac{1}{4}\left((a+b)^{2}-(a-b)^{2}\right),\]</span> to directly define covariation from variation:<span class="math display">\[[X,Y]_{t}=\frac{1}{4}\left([X+Y]_{t}-[X-Y]_{t}\right).\]</span> With this notation, <span class="math inline">\([X]\)</span> is the covariation of <span class="math inline">\(X\)</span> with itself <span class="math inline">\([X,X]\)</span>. Finally, just as <span class="math inline">\(([X]_{t})_{t \in [0,T]}\)</span> is characterized by the fact that <span class="math inline">\((X_{t}-[X]_{t})_{t \in [0,T]}\)</span> is a martingale, <span class="math inline">\(([X,Y]_{t})_{t \in [0,T]}\)</span> is characterized by the fact that <span class="math inline">\((X_{t}Y_{t}-[X,Y]_{t})_{t \in [0,T]}\)</span> is a martingale.</p>
<p>We now have:<span class="math display">\[\boxed{[\int_{0}^{\cdot}H_{u}dX_{u},\int_{0}^{\cdot}G_{u}dY_{u}]_{t}=\int_{0}^{t}H_{u}G_{u}d[X,Y]_{u},}\]</span> for the covariation of two stochastic integrals with respect to two continuous integrals.</p>
<p>To end this post, letâ€™s assume we can define for a given martingale <span class="math inline">\((X_{t})_{t \in [0,T]}\)</span> a process <span class="math inline">\((H_{t})_{t \in [0,T]}\)</span> through: <span class="math display">\[\int_{0}^{t}(H_{u})^{2}d[X]_{u}=t.\]</span> Then, by construction, the process <span class="math inline">\(Y_{t}=\int_{0}^{t}H_{u}dX_{u}\)</span> has quadratic covariation <span class="math inline">\([Y]_{t}=t\)</span>. One can show that <span class="math inline">\((Y_{t})_{t \in [0,T]}\)</span> is a Brownian motion. This construction is possible if <span class="math inline">\(([X]_{t})_{t \in [0,T]}\)</span> can be written: <span class="math display">\[[X]_{t}=\int_{0}^{t}v_{u}du\]</span> with <span class="math inline">\(v_{u}\)</span> almost everywhere strictly positive. One can then indeed pick <span class="math inline">\(H_{u}=1/(v_{u})^{1/2}\)</span>. The Brownian motion is hidden, but it is never very far.</p>
<h2 id="links">Links</h2>
<ul>
<li><a href="/assets/pdfs/2013-05-29-quadratic-variation-and-stochastic-integration.pdf">Link to pdf</a></li>
</ul>
<section class="footnotes" role="doc-endnotes">
<hr />
<ol>
<li id="fn1" role="doc-endnote"><p>See Karatzas Shreve (1991), <em>Brownian Motion and Stochastic Calculus</em>, Springer, pp 32-35.<a href="#fnref1" class="footnote-back" role="doc-backlink">â†©ï¸Ž</a></p></li>
<li id="fn2" role="doc-endnote"><p>Convergence takes place in probability uniformly with respect to time on compact time intervals.<a href="#fnref2" class="footnote-back" role="doc-backlink">â†©ï¸Ž</a></p></li>
<li id="fn3" role="doc-endnote"><p>In stochastic integration theory, we need variables to be adapted to the underlying filtration. This allows to make sense of the notion of information which is key to the interpretation of the calculus. We need also time averages and time integrals to lead to well-defined random variables. This means that, as a function of time and outcome <span class="math inline">\(\omega\)</span>, a process <span class="math inline">\(X(\cdot,\cdot)\)</span> should be measurable with respect to the product sigma algebra jointly generated by the Borel sets on the time axis and the probabilistic filtration on the <span class="math inline">\(\omega\)</span> axis.<a href="#fnref3" class="footnote-back" role="doc-backlink">â†©ï¸Ž</a></p></li>
</ol>
</section>
:ET